{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UcXIhDdm0v3e"
   },
   "source": [
    "# **PART 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Yrpv_0WRvxB",
    "outputId": "2856a431-285d-4ae4-819a-a527e5051f2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%shell\n",
    "gdown --quiet 1-IeoZDwT5wQzBUpsaS5B6vTaP-2ZBkam\n",
    "pip --quiet install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "d2NRIT-UP5AN"
   },
   "outputs": [],
   "source": [
    "COMPLAINTS_FN = 'complaints_sample.csv'\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "sc = pyspark.SparkContext.getOrCreate()\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eU0uRBgbP43-",
    "outputId": "610203bb-075f-4ca5-fffe-7fbd3404f5dd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Date received,Product,Sub-product,Issue,Sub-issue,Consumer complaint narrative,Company public response,Company,State,ZIP code,Tags,Consumer consent provided?,Submitted via,Date sent to company,Company response to consumer,Timely response?,Consumer disputed?,Complaint ID',\n",
       " '2015-12-31,Bank account or service,Checking account,\"Making/receiving payments, sending money\",,,,FIRSTBANK PUERTO RICO,PR,00902,Older American,N/A,Referral,2016-02-04,Closed with explanation,Yes,No,1723943']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = sc.textFile(COMPLAINTS_FN, use_unicode=True).cache()\n",
    "dt.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YREZEzItP4zd",
    "outputId": "0a82da96-c63b-4def-c90a-a6407941b41c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date received,Product,Sub-product,Issue,Sub-issue,Consumer complaint narrative,Company public response,Company,State,ZIP code,Tags,Consumer consent provided?,Submitted via,Date sent to company,Company response to consumer,Timely response?,Consumer disputed?,Complaint ID\n",
      ".\n",
      "2015-12-31,Bank account or service,Checking account,\"Making/receiving payments, sending money\",,,,FIRSTBANK PUERTO RICO,PR,00902,Older American,N/A,Referral,2016-02-04,Closed with explanation,Yes,No,1723943\n",
      ".\n",
      "2016-03-15,Bank account or service,Other bank product/service,Problems caused by my funds being low,,,,FIRSTBANK PUERTO RICO,PR,00926,,Consent not provided,Web,2016-03-15,Closed with explanation,Yes,No,1833740\n",
      ".\n",
      "2016-10-24,Bank account or service,Checking account,\"Account opening, closing, or management\",,\"In the month of XX/XX/2015, my email address ( XXXX ) was hacked and used to send messages to people associated with my business. At that time, transactions for the purchase and sales of products were made. The hacker forged the identities of our customers and suppliers, creating email addresses similar to those recorded in our email account, which they then used to communicate with our customers, our suppliers, and us. \n",
      ".\n",
      "\n",
      ".\n",
      "The hackers fraudulently and inexplicably opened XXXX ( XXXX ) accounts with WELLS FARGO BANK in XXXX XXXX, California.\",Company has responded to the consumer and the CFPB and chooses not to provide a public response,WELLS FARGO & COMPANY,PR,00969,,Consent provided,Web,2016-12-28,Closed with non-monetary relief,Yes,No,2175792\n",
      ".\n",
      "2017-09-08,Checking or savings account,Checking account,Managing an account,Deposits and withdrawals,,,Comerica,TX,77551,,N/A,Referral,2017-11-07,Closed with explanation,Yes,N/A,2668920\n",
      ".\n",
      "2018-09-19,Checking or savings account,Checking account,Problem with a lender or other company charging your account,Money was taken from your account on the wrong day or for the wrong amount,,Company has responded to the consumer and the CFPB and chooses not to provide a public response,WELLS FARGO & COMPANY,FL,33326,,Consent not provided,Web,2018-09-19,Closed with explanation,Yes,N/A,3023007\n",
      ".\n",
      "2018-12-04,Checking or savings account,Checking account,Managing an account,Deposits and withdrawals,,Company has responded to the consumer and the CFPB and chooses not to provide a public response,\"BANK OF AMERICA, NATIONAL ASSOCIATION\",MI,48146,,N/A,Referral,2018-12-11,Closed with explanation,Yes,N/A,3093436\n",
      ".\n",
      "2018-12-05,Checking or savings account,Other banking product or service,Managing an account,Deposits and withdrawals,\"On the XX/XX/2018, I was unable to access funds in my account using my debit card at a car dealership, despite adequate funds being available in my account. The amount was {$12000.00}. \n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for x in dt.take(10):\n",
    "    print(x)\n",
    "    print('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6WTxjr8iP4wM"
   },
   "outputs": [],
   "source": [
    "def combine(part_id,rows):\n",
    "    if part_id == 0:\n",
    "        next(rows)\n",
    "    reserve = ''\n",
    "    for row in rows:\n",
    "        if row[:4].isnumeric()==True:\n",
    "            if len(reserve)>0:\n",
    "                yield reserve\n",
    "            reserve = row\n",
    "        else:\n",
    "            reserve+=row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hc42KCWKP4tA"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def modify(row):\n",
    "    pattern = r'\"[^\"]*\"'\n",
    "    matches = re.finditer(pattern,row)\n",
    "    positions = []\n",
    "    for match in matches:\n",
    "        positions.append((match.start(), match.end()))\n",
    "    quot_modified = []\n",
    "    if len(positions)>0:\n",
    "        for x in positions:\n",
    "            target_quot = row[x[0]:x[1]]\n",
    "            replacement_quot = re.sub(\",\",\"_\", target_quot) \n",
    "            quot_modified.append((replacement_quot,x))\n",
    "        output_row = row\n",
    "        for y in quot_modified:\n",
    "            output_row = output_row[:y[1][0]]+y[0]+output_row[y[1][1]:]\n",
    "        return output_row.lower()\n",
    "    else:\n",
    "        return row.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aKEB-TEyP4Uw",
    "outputId": "45aa7baf-8144-4587-ddf7-a27c763b1656"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('2015', 'bank account or service', 'firstbank puerto rico'), 1),\n",
       " (('2016', 'bank account or service', 'firstbank puerto rico'), 1),\n",
       " (('2016', 'bank account or service', 'wells fargo & company'), 1),\n",
       " (('2017', 'checking or savings account', 'comerica'), 1),\n",
       " (('2018', 'checking or savings account', 'wells fargo & company'), 1),\n",
       " (('2018',\n",
       "   'checking or savings account',\n",
       "   '\"bank of america_ national association\"'),\n",
       "  1),\n",
       " (('2018', 'checking or savings account', 'hsbc north america holdings inc.'),\n",
       "  1),\n",
       " (('2018', 'checking or savings account', 'jpmorgan chase & co.'), 1),\n",
       " (('2018', 'checking or savings account', 'navy federal credit union'), 1),\n",
       " (('2018', 'checking or savings account', 'jpmorgan chase & co.'), 1)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task1 = dt.mapPartitionsWithIndex(combine)\\\n",
    "        .map(modify)\\\n",
    "        .map(lambda x: x.split(','))\\\n",
    "        .map(lambda x: ((x[0][:4],x[1],x[7]),1))\n",
    "task1.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FPGAJgpLNWWD"
   },
   "outputs": [],
   "source": [
    "check = task1.filter(lambda x: x[0][0] in ['2018'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4AEdzTM8Sg-0",
    "outputId": "0e69e291-93a1-4bd2-eaa4-b419501b1f76"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bank account or service,2015,1,1,100',\n",
       " 'bank account or service,2016,2,2,50',\n",
       " 'checking or savings account,2017,1,1,100',\n",
       " 'checking or savings account,2018,20,10,25',\n",
       " 'checking or savings account,2019,461,72,13',\n",
       " 'checking or savings account,2020,3,3,33',\n",
       " 'consumer loan,2015,1,1,100',\n",
       " 'consumer loan,2016,1,1,100',\n",
       " 'consumer loan,2017,1,1,100',\n",
       " 'credit card or prepaid card,2017,1,1,100',\n",
       " 'credit card or prepaid card,2018,27,12,33',\n",
       " 'credit card or prepaid card,2019,437,42,15',\n",
       " 'credit card or prepaid card,2020,13,10,23',\n",
       " 'credit card,2016,4,4,25',\n",
       " 'credit card,2017,1,1,100',\n",
       " 'credit reporting_ credit repair services_ or other personal consumer reports,2017,7,5,29',\n",
       " 'credit reporting_ credit repair services_ or other personal consumer reports,2018,238,22,56',\n",
       " 'credit reporting_ credit repair services_ or other personal consumer reports,2019,3113,203,50',\n",
       " 'credit reporting_ credit repair services_ or other personal consumer reports,2020,144,10,51',\n",
       " 'debt collection,2015,4,3,50']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_quot(row):\n",
    "    if row[0][0].isalpha():\n",
    "        return ','.join(row)\n",
    "    else:\n",
    "        out = [row[0][1:-1],row[1],row[2],row[3],row[4]]\n",
    "        return ','.join(out)\n",
    "outputTask1 = task1.groupByKey()\\\n",
    "        .mapValues(lambda x: sum(x))\\\n",
    "        .map(lambda x: ((x[0][0],x[0][1]),(x[1],1)))\\\n",
    "        .groupByKey()\\\n",
    "        .mapValues(lambda x: (sum([a[0] for a in x]),max([a[0] for a in x]),sum([a[1] for a in x])))\\\n",
    "        .map(lambda x: [x[0][1],x[0][0],str(x[1][0]),str(x[1][2]),str(round(100*x[1][1]/x[1][0],0))[:-2]])\\\n",
    "        .map(remove_quot)\\\n",
    "        .sortBy(lambda x: x[:])\n",
    "outputTask1.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xWkqdqzzOtU9"
   },
   "outputs": [],
   "source": [
    "# delate folder\n",
    "#!rm -rf 'report.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dTZnCpbhSrf5"
   },
   "source": [
    "# part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 558
    },
    "id": "qz_30RDSkIW6",
    "outputId": "c0bb863d-6f84-4b8b-d87b-6e04e54796d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting google-cloud-dataproc\n",
      "  Downloading google_cloud_dataproc-5.4.1-py2.py3-none-any.whl (307 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.5/307.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /usr/local/lib/python3.9/dist-packages (from google-cloud-dataproc) (1.22.2)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /usr/local/lib/python3.9/dist-packages (from google-cloud-dataproc) (3.20.3)\n",
      "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0 in /usr/local/lib/python3.9/dist-packages (from google-cloud-dataproc) (2.11.0)\n",
      "Collecting grpc-google-iam-v1<1.0.0dev,>=0.12.4\n",
      "  Downloading grpc_google_iam_v1-0.12.6-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.9/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-dataproc) (2.27.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.9/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-dataproc) (1.59.0)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=2.14.1 in /usr/local/lib/python3.9/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-dataproc) (2.17.2)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /usr/local/lib/python3.9/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-dataproc) (1.48.2)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.9/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-dataproc) (1.53.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3.0dev,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-dataproc) (5.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3.0dev,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-dataproc) (4.9)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3.0dev,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-dataproc) (1.16.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3.0dev,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-dataproc) (0.2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-dataproc) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-dataproc) (3.4)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-dataproc) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-dataproc) (1.26.15)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-dataproc) (0.4.8)\n",
      "Installing collected packages: grpc-google-iam-v1, google-cloud-dataproc\n",
      "Successfully installed google-cloud-dataproc-5.4.1 grpc-google-iam-v1-0.12.6\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "google"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install google-cloud-dataproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pysIeU5ZkXC_",
    "outputId": "2139541b-90a1-4fcd-e84d-2e5925fcc8e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to the following link in your browser:\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=32555940559.apps.googleusercontent.com&redirect_uri=https%3A%2F%2Fsdk.cloud.google.com%2Fauthcode.html&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fappengine.admin+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcompute+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth&state=dVHPqZDlY3OjaqlZhoygLdo4SZceEL&prompt=consent&access_type=offline&code_challenge=cMHT2OYMiGzblktM0zRoa4m98QwrT6BuojpoLro4yF8&code_challenge_method=S256\n",
      "\n",
      "Enter authorization code: 4/0AVHEtk4gS6gjABfhHOo_k2yyM2DBx5-Vic-JhXaAX95EourVw8IJRIzfmPCAwQd9qfFQ8w\n",
      "\n",
      "You are now logged in as [zhiyuan.jin1201@gmail.com].\n",
      "Your current project is [None].  You can change this setting by running:\n",
      "  $ gcloud config set project PROJECT_ID\n"
     ]
    }
   ],
   "source": [
    "!gcloud auth login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ixWzR6eblxMt",
    "outputId": "ffa87ac8-9283-472e-a4c4-0a55e14e3c44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ID            NAME              PROJECT_NUMBER\n",
      "bda-12345             bda-12345         369021837527\n",
      "directed-will-384217  My First Project  913257743120\n"
     ]
    }
   ],
   "source": [
    "!gcloud projects list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ka9IZeDXl9IF",
    "outputId": "820e7be7-09a4-4abc-ac80-ae04a2a20f04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "Updated property [compute/region].\n",
      "Updated property [compute/zone].\n",
      "Updated property [dataproc/region].\n"
     ]
    }
   ],
   "source": [
    "!gcloud config set project bda-12345\n",
    "!gcloud config set compute/region us-west1\n",
    "!gcloud config set compute/zone us-west1-a\n",
    "!gcloud config set dataproc/region us-west1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vcIMTtOOmoMd",
    "outputId": "aca0b73a-c5be-4a4b-9f9e-dc44a8e21ace"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting on operation [projects/bda-12345/regions/us-west1/operations/5a653e72-d97f-3d1b-a948-3a30d0e45cef].\n",
      "\n",
      "\u001b[1;33mWARNING:\u001b[0m Consider using Auto Zone rather than selecting a zone manually. See https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone\n",
      "\u001b[1;33mWARNING:\u001b[0m Failed to validate permissions required for default service account: '369021837527-compute@developer.gserviceaccount.com'. Cluster creation could still be successful if required permissions have been granted to the respective service accounts as mentioned in the document https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/service-accounts#dataproc_service_accounts_2. This could be due to Cloud Resource Manager API hasn't been enabled in your project '369021837527' before or it is disabled. Enable it by visiting 'https://console.developers.google.com/apis/api/cloudresourcemanager.googleapis.com/overview?project=369021837527'.\n",
      "\u001b[1;33mWARNING:\u001b[0m For PD-Standard without local SSDs, we strongly recommend provisioning 1TB or larger to ensure consistently high I/O performance. See https://cloud.google.com/compute/docs/disks/performance for information on disk I/O performance.\n",
      "Created [https://dataproc.googleapis.com/v1/projects/bda-12345/regions/us-west1/clusters/bdm-hw3] Cluster placed in zone [us-west1-a].\n"
     ]
    }
   ],
   "source": [
    "!gcloud dataproc clusters create bdm-hw3 --enable-component-gateway --region us-west1 --zone us-west1-a --master-machine-type n1-standard-4 --master-boot-disk-size 500 --num-workers 2 --worker-machine-type n1-standard-4 --worker-boot-disk-size 500 --image-version 2.0-debian10 --project bda-12345"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hYXsIhZ15tw1",
    "outputId": "9a2ad0aa-82ae-4b6b-bcce-5516a9a668de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME     PLATFORM  WORKER_COUNT  PREEMPTIBLE_WORKER_COUNT  STATUS   ZONE        SCHEDULED_DELETE\n",
      "bdm-hw3  GCE       2                                       RUNNING  us-west1-a\n"
     ]
    }
   ],
   "source": [
    "!gcloud dataproc clusters list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TeEbxnVlq7pr",
    "outputId": "9eacbcc2-bdfa-4cae-9aed-6af6bc9943b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting BDM_HW3_23499013_Jin.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile BDM_HW3_23499013_Jin.py\n",
    "#!/usr/bin/python\n",
    "COMPLAINTS = 'gs://bdma/data/complaints.csv'\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "sc = pyspark.SparkContext.getOrCreate()\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "dt = sc.textFile(COMPLAINTS, use_unicode=True).cache()\n",
    "\n",
    "def combine(part_id,rows):\n",
    "    if part_id == 0:\n",
    "        next(rows)\n",
    "    reserve = ''\n",
    "    for row in rows:\n",
    "        if row[:4].isnumeric()==True:\n",
    "            if len(reserve)>0:\n",
    "                yield reserve\n",
    "            reserve = row\n",
    "        else:\n",
    "            reserve+=row\n",
    "\n",
    "import re\n",
    "def modify(row):\n",
    "    pattern = r'\"[^\"]*\"'\n",
    "    matches = re.finditer(pattern,row)\n",
    "    positions = []\n",
    "    for match in matches:\n",
    "        positions.append((match.start(), match.end()))\n",
    "    # change all the commas-',' in the quotation mark to _, so that I can split with comma later on.\n",
    "    # and use the modified sentence in the quotation mark to replace the original sentence in the quotation mark.\n",
    "    quot_modified = []\n",
    "    if len(positions)>0:\n",
    "        for x in positions:\n",
    "            target_quot = row[x[0]:x[1]]\n",
    "            replacement_quot = re.sub(\",\",\"_\", target_quot) \n",
    "            quot_modified.append((replacement_quot,x))\n",
    "        output_row = row\n",
    "        for y in quot_modified:\n",
    "            output_row = output_row[:y[1][0]]+y[0]+output_row[y[1][1]:]\n",
    "        return output_row.lower()\n",
    "    else:\n",
    "        return row.lower()\n",
    "\n",
    "task1 = dt.mapPartitionsWithIndex(combine)\\\n",
    "        .map(modify)\\\n",
    "        .map(lambda x: x.split(','))\\\n",
    "        .map(lambda x: ((x[0][:4],x[1],x[7]),1))\n",
    "\n",
    "def remove_quot(row):\n",
    "    if row==none:\n",
    "      pass\n",
    "    elif row[0][0].isalpha():\n",
    "        return ','.join(row)\n",
    "    else:\n",
    "        out = [row[0][1:-1],row[1],row[2],row[3],row[4]]\n",
    "        return ','.join(out)\n",
    "        \n",
    "outputTask1 = task1.groupByKey()\\\n",
    "        .mapValues(lambda x: sum(x))\\\n",
    "        .map(lambda x: ((x[0][0],x[0][1]),(x[1],1)))\\\n",
    "        .groupByKey()\\\n",
    "        .mapValues(lambda x: (sum([a[0] for a in x]),max([a[0] for a in x]),sum([a[1] for a in x])))\\\n",
    "        .map(lambda x: [x[0][1],x[0][0],str(x[1][0]),str(x[1][2]),str(round(100*x[1][1]/x[1][0],0))[:-2]])\\\n",
    "        .map(remove_quot)\\\n",
    "        .sortBy(lambda x: x)\n",
    "outputTask1.saveAsTextFile('report.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m5lLA-gDdhGF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_vJLVnch4-5t",
    "outputId": "929464d7-a069-4e52-ae2f-44c19a3d4afc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job [de33e22e86ca4ba5bdefd07d1a57df5e] submitted.\n",
      "Waiting for job output...\n",
      "23/04/20 02:14:55 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n",
      "23/04/20 02:14:55 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n",
      "23/04/20 02:14:55 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "23/04/20 02:14:55 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n",
      "23/04/20 02:14:55 INFO org.sparkproject.jetty.util.log: Logging initialized @3298ms to org.sparkproject.jetty.util.log.Slf4jLog\n",
      "23/04/20 02:14:55 INFO org.sparkproject.jetty.server.Server: jetty-9.4.40.v20210413; built: 2021-04-13T20:42:42.668Z; git: b881a572662e1943a14ae12e7e1207989f218b74; jvm 1.8.0_362-b09\n",
      "23/04/20 02:14:55 INFO org.sparkproject.jetty.server.Server: Started @3417ms\n",
      "23/04/20 02:14:55 INFO org.sparkproject.jetty.server.AbstractConnector: Started ServerConnector@32bcf606{HTTP/1.1, (http/1.1)}{0.0.0.0:34853}\n",
      "23/04/20 02:14:56 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at bdm-hw3-m/10.138.0.19:8032\n",
      "23/04/20 02:14:56 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at bdm-hw3-m/10.138.0.19:10200\n",
      "23/04/20 02:14:58 INFO org.apache.hadoop.conf.Configuration: resource-types.xml not found\n",
      "23/04/20 02:14:58 INFO org.apache.hadoop.yarn.util.resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "23/04/20 02:14:58 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1681953496164_0016\n",
      "23/04/20 02:14:59 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at bdm-hw3-m/10.138.0.19:8030\n",
      "23/04/20 02:15:01 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.\n",
      "23/04/20 02:15:03 INFO org.apache.hadoop.mapred.FileInputFormat: Total input files to process : 1\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/de33e22e86ca4ba5bdefd07d1a57df5e/BDM_HW3_23499013_Jin.py\", line 65, in <module>\n",
      "    outputTask1.saveAsTextFile('report.csv')\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 1826, in saveAsTextFile\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1304, in __call__\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 111, in deco\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\", line 326, in get_return_value\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o104.saveAsTextFile.\n",
      ": org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs://bdm-hw3-m/user/root/report.csv already exists\n",
      "\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\n",
      "\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:298)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:71)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1090)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1088)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1061)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1008)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1007)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:964)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:962)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1578)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n",
      "\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1578)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1564)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n",
      "\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1564)\n",
      "\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile(JavaRDDLike.scala:551)\n",
      "\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile$(JavaRDDLike.scala:550)\n",
      "\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "23/04/20 02:15:03 INFO org.sparkproject.jetty.server.AbstractConnector: Stopped Spark@32bcf606{HTTP/1.1, (http/1.1)}{0.0.0.0:0}\n",
      "\u001b[1;31mERROR:\u001b[0m (gcloud.dataproc.jobs.submit.pyspark) Job [de33e22e86ca4ba5bdefd07d1a57df5e] failed with error:\n",
      "Google Cloud Dataproc Agent reports job failure. If logs are available, they can be found at:\n",
      "https://console.cloud.google.com/dataproc/jobs/de33e22e86ca4ba5bdefd07d1a57df5e?project=bda-12345&region=us-west1\n",
      "gcloud dataproc jobs wait 'de33e22e86ca4ba5bdefd07d1a57df5e' --region 'us-west1' --project 'bda-12345'\n",
      "https://console.cloud.google.com/storage/browser/dataproc-staging-us-west1-369021837527-rmrmdi8b/google-cloud-dataproc-metainfo/dd0a1695-ca33-4577-85c6-467f60d8451e/jobs/de33e22e86ca4ba5bdefd07d1a57df5e/\n",
      "gs://dataproc-staging-us-west1-369021837527-rmrmdi8b/google-cloud-dataproc-metainfo/dd0a1695-ca33-4577-85c6-467f60d8451e/jobs/de33e22e86ca4ba5bdefd07d1a57df5e/driveroutput\n"
     ]
    }
   ],
   "source": [
    "!gcloud dataproc jobs submit pyspark --cluster bdm-hw3 BDM_HW3_23499013_Jin.py -- gs://bdma/data/complaints.csv gs://bdma/shared/2023_spring/HW3/23499013_Jin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GSc4tuXZ5pjp",
    "outputId": "aa843847-774b-4c3b-e492-a2db2b7c23e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://bdma/shared/2023_spring/HW3/\n",
      "gs://bdma/shared/2023_spring/HW3/14211712_Salas/\n",
      "gs://bdma/shared/2023_spring/HW3/15304719_Graig/\n",
      "gs://bdma/shared/2023_spring/HW3/23513476_Gabr/\n",
      "gs://bdma/shared/2023_spring/HW3/23621671_Kannam/\n",
      "gs://bdma/shared/2023_spring/HW3/23668189_Matuszewski/\n",
      "gs://bdma/shared/2023_spring/HW3/23703839_Feng/\n",
      "gs://bdma/shared/2023_spring/HW3/23735863_Ghimire/\n",
      "gs://bdma/shared/2023_spring/HW3/24363838_Lau/\n",
      "gs://bdma/shared/2023_spring/HW3/24369480_Chandani/\n",
      "gs://bdma/shared/2023_spring/HW3/24369705_Khandale/\n",
      "gs://bdma/shared/2023_spring/HW3/24369977_Gulve/\n",
      "gs://bdma/shared/2023_spring/HW3/24373710_Uddin/\n",
      "gs://bdma/shared/2023_spring/HW3/24379388_Quadri/\n",
      "gs://bdma/shared/2023_spring/HW3/24379541_Desai/\n",
      "gs://bdma/shared/2023_spring/HW3/24379542_doroudian/\n",
      "gs://bdma/shared/2023_spring/HW3/24438996_Radaelli/\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls gs://bdma/shared/2023_spring/HW3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MkiRKMrv5drE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
